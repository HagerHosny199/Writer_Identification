{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming conventions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# as we write python code then:\n",
    "# 1- variables : lowercase seperated by underscore (my_variable)\n",
    "# 2- methods : lowercase seperated by underscore (my_function)\n",
    "# 3- constants : ALLCAPITAL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import OS to share the project dir\n",
    "import os\n",
    "os.chdir('F:\\\\fourth_year\\Pattern_Classification\\project\\Writer_Identification') \n",
    "\n",
    "#import important packages\n",
    "from IAM_loader import IAM_loader\n",
    "#import tensorflow as tf\n",
    "#import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import cv2 as cv2\n",
    "from PIL import Image\n",
    "import math\n",
    "from skimage import feature\n",
    "\n",
    "#packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "#allows charts to appear in the notebook \n",
    "%matplotlib inline\n",
    "\n",
    "#ignoring warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we use IAM handwriting database download it first\n",
    "# here we will read the images and get the important info about the images \n",
    "# we divide the available images into (train,cv and test)\n",
    "# division will be depend on the number of the images \n",
    "\n",
    "training_data=[] # only the train \n",
    "training_data_threshold=[] #training black images \n",
    "validation_data=[] # cross validation \n",
    "test_data=[] # test images \n",
    "\n",
    "WIDTH=2175 #image width\n",
    "HEIGHT=2304 #image height\n",
    "path=\"../dataset/output2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reading_and_preprocess():\n",
    "    loader=IAM_loader('../dataset')\n",
    "    training_data,test_data,validation_data = loader.split_data()\n",
    "    cv2.imwrite('../dataset/lines199.png',training_data[0][0])\n",
    "    #writting the data \n",
    "    training_data_threshold=training_data\n",
    "    for i in range(len(training_data)):\n",
    "            #path_output = os.path.join(path,str(i)+'.png')\n",
    "            temp=training_data[i][0]\n",
    "            _,image= cv2.threshold(temp,0,255,cv2.THRESH_BINARY_INV)\n",
    "            #cv2.imwrite(path_output,training_data[i][0])\n",
    "            training_data_threshold[i][0]=image\n",
    "    return training_data,test_data,validation_data,training_data_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crop the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get the window that contains the words \n",
    "def crop_image(image,y1,y2,x1,x2):\n",
    "    image=image[y1:y2,x1:x2]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resize Image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#note : use WIDTH and HEIGHT \n",
    "def resize_image (img):\n",
    "    img=cv2.resize(img, dsize=(HEIGHT, WIDTH), interpolation=cv2.INTER_CUBIC)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect lines to get the region of handwritten document: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_lines(img):\n",
    "    horizontal_lines=[]\n",
    "    ret_lines=[] \n",
    "    #apply canny \n",
    "    edges = cv2.Canny(img,50,150,apertureSize = 3)\n",
    "    #dilation\n",
    "    kernel = np.ones((5,5), np.uint8)\n",
    "    img_dilation = cv2.dilate(edges, kernel, iterations=1)\n",
    "    #hough line transform\n",
    "    lines = cv2.HoughLinesP(img_dilation, 1, np.pi/180,200, maxLineGap=1,minLineLength=img.shape[1]/2)\n",
    "    #getting the horizontal lines \n",
    "    for i in range(len(lines)):\n",
    "           for line in lines[i]:\n",
    "                pt1 = (line[0],line[1])\n",
    "                pt2 = (line[2],line[3])\n",
    "                if (line[1]-line[3])==0 : #print((line[2]-line[3])//(line[0]-line[1]))\n",
    "                        cv2.line(img, pt1, pt2, (255,255,255), 3) \n",
    "                        horizontal_lines.append(line)\n",
    "    #sort lines to eleminate the closed lines \n",
    "    horizontal_lines=sorted(horizontal_lines,key=lambda x: x[1])\n",
    "    pt=horizontal_lines[0]\n",
    "    ret_lines.append(pt)\n",
    "    #loop to retrive the included lines \n",
    "    for i in range(1,len(horizontal_lines)):\n",
    "        if abs(horizontal_lines[i][1]-pt[1])>100:\n",
    "            #print(\"included\",horizontal_lines[i])\n",
    "            pt=horizontal_lines[i]\n",
    "            ret_lines.append(pt)\n",
    "    return ret_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_region():\n",
    "    for i in range(len(training_data)):\n",
    "        img=training_data[i][0]\n",
    "        lines=detect_lines(img)\n",
    "        print(len(lines))\n",
    "        print(img.shape,lines[1][1]+10,lines[2][1]-3,math.floor(lines[1][0]/4),img.shape[1]-1)\n",
    "        cropped_img=crop_image(img,lines[1][1]+10,lines[2][1]-3,math.floor(lines[1][0]/4),img.shape[1]-1)\n",
    "        cropped_img=resize_image(cropped_img)\n",
    "        training_data[i][0]=cropped_img\n",
    "        cv2.imwrite('../dataset/output/cropped'+str(i)+'.png',cropped_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Getting Connected Comp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def connected_comp(img):\n",
    "    #ret, labels = cv2.connectedComponents(img)\n",
    "    output=cv2.connectedComponentsWithStats(img, 4, cv2.CV_32S)\n",
    "    ret=output[0]\n",
    "    labels=output[1]\n",
    "    stat=output[2]\n",
    "    # Map component labels to hue val\n",
    "    label_hue = np.uint8(179*labels/np.max(labels))\n",
    "    blank_ch = 255*np.ones_like(label_hue)\n",
    "    labeled_img = cv2.merge([label_hue, blank_ch, blank_ch])\n",
    "\n",
    "    # cvt to BGR for display\n",
    "    labeled_img = cv2.cvtColor(labeled_img, cv2.COLOR_HSV2BGR)\n",
    "\n",
    "    # set bg label to black\n",
    "    labeled_img[label_hue==0] = 0\n",
    "\n",
    "    #cv2.imwrite('labeled.png', labeled_img)\n",
    "    return ret,labels,stat,labeled_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### SIFT :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sift_fun (img):\n",
    "    #img = cv.imread(img_path)\n",
    "    #gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    kp = sift.detect(img,None)\n",
    "    #img=cv.drawKeypoints(gray,kp,img)\n",
    "    #cv.imwrite('sift_keypoints.jpg',img)\n",
    "    return kp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texture Extraction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def texture_extraction (labels,image,num_labels,stats):\n",
    "    #define new image \n",
    "    output=np.zeros(shape=(image.shape))\n",
    "    visited=np.zeros(shape=(labels.shape))\n",
    "    h=10\n",
    "    max_height_old=0\n",
    "    max_height_new=0\n",
    "    flag=0\n",
    "    y_old=0\n",
    "    avg_height=0\n",
    "    num=0\n",
    "    #loop in the rows and cols to get the labels \n",
    "    for i in range(labels.shape[0]):#height ->row by row\n",
    "        max_height_new=0\n",
    "        for j in range (labels.shape[1]):#widths -> col by col\n",
    "            if visited[i][j]!=1 and labels[i][j]!=0:\n",
    "                label=labels[i][j]\n",
    "                # retrieving the width of the bounding box of the component\n",
    "                width = stats[label, cv2.CC_STAT_WIDTH]\n",
    "                # retrieving the height of the bounding box of the component\n",
    "                height = stats[label, cv2.CC_STAT_HEIGHT]\n",
    "                # retrieving the leftmost coordinate of the bounding box of the component\n",
    "                x = stats[label, cv2.CC_STAT_LEFT] #x->col\n",
    "                # retrieving the topmost coordinate of the bounding box of the component\n",
    "                y = stats[label, cv2.CC_STAT_TOP] #y->row\n",
    "                #rgb = cv2.rectangle(rgb, (x, y+height), (x+width, y), (255,160,122),1)\n",
    "                if width>10:\n",
    "                    #print(\"h=\",h,\"x=\",x,\"y=\",y,\"height=\",height,\"i=\",i,\"width=\",width,i,\"j=\",j,\"lbel=\",label,\"num=\",num)\n",
    "                    #copy the box of the label \n",
    "                    for k in range(h,h+height):\n",
    "                        for l in range (x,x+width):\n",
    "                            if image[y+k-h][l].all() ==0:\n",
    "                                    continue\n",
    "                            else :\n",
    "                                 output[k][l]=image[y+k-h][l]\n",
    "                            \n",
    "                      #  output[h:h+height,x:x+width]=image[y:y+height,x:x+width]\n",
    "                    visited[y:y+height,x:x+width]=1\n",
    "                    if height>max_height_new:\n",
    "                        max_height_new=height\n",
    "                    #calculae avg height \n",
    "                    avg_height+=height\n",
    "                    num+=1\n",
    "                    \n",
    "                #h=math.floor((h+5)/2)\n",
    "        if flag==0 and max_height_new>0:\n",
    "            max_height_old=max_height_new\n",
    "            y_old=i\n",
    "            flag=1\n",
    "        elif num>0 and flag==1 and i>=(max_height_old+y_old):\n",
    "            h=math.ceil(h+(avg_height/(2*num)))\n",
    "            max_height_old=max_height_new\n",
    "            y_old=i\n",
    "            avg_height=0\n",
    "            num=0\n",
    "            \n",
    "   \n",
    "    return output,h+100         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_nine_texture(img):\n",
    "    images=[]\n",
    "    for i in range(9):\n",
    "        images.append(img[i*128:(i+1)*127,i*256:(i+1)*255])\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_texture():\n",
    "    text_images=[]\n",
    "    for i in range(len(training_data_threshold)):\n",
    "        ret,labels,stats,image=connected_comp(training_data_threshold[i][0])\n",
    "        output,h=texture_extraction(labels,training_data_threshold[i][0],ret,stats)\n",
    "        print(i,h)\n",
    "        img=crop_image(output,0,1100,0,output.shape[1])\n",
    "        #cv2.imwrite(\"../dataset/output/texture\"+str(i)+\".png\",img)\n",
    "        images=get_nine_texture(img)\n",
    "        text_images.append(images)\n",
    "    return text_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LBP :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LBP(text_images):\n",
    "    hist_=[]\n",
    "    labels=[]\n",
    "    for i in range(len(training_data)):\n",
    "        for j in range(9):\n",
    "            # compute the Local Binary Pattern representation\n",
    "            # of the image, and then use the LBP representation\n",
    "            # to build the histogram of patterns\n",
    "            lbp = feature.local_binary_pattern(text_images[i][j],8,1, method=\"edge\") #num_of_pointts=24 radius=8\n",
    "            (hist, _) = np.histogram(lbp.ravel(),\n",
    "            bins=np.arange(0, 8+ 3),range=(0,8+ 2))\n",
    " \n",
    "            # normalize the histogram\n",
    "            hist = hist.astype(\"float\")\n",
    "            hist_.append(hist)\n",
    "            labels.append(training_data[i][1])\n",
    "    return hist_,labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Module:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-Bayes Classifer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bayes_classifer():\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVM_fun(x,y,x_test):\n",
    "    clf = svm.SVC(gamma=0.1,kernel='rbf')\n",
    "    clf.fit(x, y) \n",
    "    y_test=clf.predict(x_test)\n",
    "    return y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-Linear/Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def regression():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- K-NN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn(X,y,test):\n",
    "    neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "    neigh.fit(X, y) \n",
    "    labels_test=neigh.predict(test)\n",
    "    return labels_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Neural Network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network():\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy():\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document a01-000u to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-035 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "Adding document r06-090 to the training data\n",
      "3\n",
      "(3542, 2479) 614 2786 217 2478\n",
      "3\n",
      "(3542, 2479) 716 2788 179 2478\n",
      "3\n",
      "(3542, 2479) 664 2786 183 2478\n",
      "3\n",
      "(3542, 2479) 663 2785 208 2478\n",
      "3\n",
      "(3542, 2479) 664 2786 198 2478\n",
      "3\n",
      "(3542, 2479) 666 2787 165 2478\n",
      "3\n",
      "(3542, 2479) 714 2787 220 2478\n",
      "3\n",
      "(3542, 2479) 714 2786 171 2478\n",
      "3\n",
      "(3542, 2479) 663 2785 177 2478\n",
      "3\n",
      "(3542, 2479) 711 2784 211 2478\n",
      "3\n",
      "(3542, 2479) 563 2788 84 2478\n",
      "3\n",
      "(3542, 2479) 616 2790 84 2478\n",
      "3\n",
      "(3542, 2479) 714 2788 84 2478\n",
      "3\n",
      "(3542, 2479) 664 2789 83 2478\n",
      "3\n",
      "(3542, 2479) 616 2790 80 2478\n",
      "3\n",
      "(3542, 2479) 616 2789 86 2478\n",
      "3\n",
      "(3542, 2479) 566 2789 83 2478\n",
      "3\n",
      "(3542, 2479) 616 2790 83 2478\n",
      "3\n",
      "(3542, 2479) 616 2791 81 2478\n",
      "3\n",
      "(3542, 2479) 615 2788 84 2478\n",
      "3\n",
      "(3542, 2479) 667 2791 85 2478\n",
      "3\n",
      "(3542, 2479) 668 2793 81 2478\n",
      "3\n",
      "(3542, 2479) 567 2792 83 2478\n",
      "3\n",
      "(3542, 2479) 616 2791 189 2478\n",
      "3\n",
      "(3542, 2479) 566 2790 82 2478\n",
      "3\n",
      "(3542, 2479) 665 2790 81 2478\n",
      "3\n",
      "(3542, 2479) 665 2790 105 2478\n",
      "3\n",
      "(3542, 2479) 613 2787 82 2478\n",
      "3\n",
      "(3542, 2479) 614 2789 183 2478\n",
      "3\n",
      "(3542, 2479) 613 2787 81 2478\n",
      "0 550\n",
      "1 848\n",
      "2 782\n",
      "3 791\n",
      "4 724\n",
      "5 692\n",
      "6 859\n",
      "7 737\n",
      "8 797\n",
      "9 944\n",
      "10 614\n",
      "11 813\n",
      "12 1181\n",
      "13 1155\n",
      "14 902\n",
      "15 914\n",
      "16 956\n",
      "17 934\n",
      "18 1023\n",
      "19 869\n",
      "20 824\n"
     ]
    }
   ],
   "source": [
    "#here we will call the previous methods in a certain sequence \n",
    "#in this step we read the data and doing some of the preprocessing \n",
    "#1-reading &splitting into training ,testing & CV\n",
    "#2-binaraization using OTSU\n",
    "#3-segmentation of the handwritten document\n",
    "#4-writing output to a certain folder \n",
    "#5-apply inverse binarization\n",
    "training_data,test_data,validation_data,training_data_threshold=reading_and_preprocess() \n",
    "get_region()\n",
    "text_images=get_texture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist,labels=LBP(text_images)\n",
    "hist=np.array(hist)\n",
    "#hist.reshape(len(hist),len((hist[0])))\n",
    "count=0\n",
    "labels_test=SVM_fun(hist,labels,hist)\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]==labels_test[i]:\n",
    "        count+=1\n",
    "print(\"acc=\",count/(len(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_test=knn(hist,labels,hist)\n",
    "count=0\n",
    "for i in range(len(labels)):\n",
    "    if labels[i]==labels_test[i]:\n",
    "        count+=1\n",
    "print(\"acc=\",count/(len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(3.4.3) C:\\projects\\opencv-python\\opencv_contrib\\modules\\xfeatures2d\\src\\sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'cv::xfeatures2d::SIFT::create'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-0e1aa8810664>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msift_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainin_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-38-18e0fc449529>\u001b[0m in \u001b[0;36msift_fun\u001b[1;34m(img)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;31m#img = cv.imread(img_path)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;31m#gray= cv.cvtColor(img,cv.COLOR_BGR2GRAY)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0msift\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxfeatures2d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSIFT_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mkp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msift\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#img=cv.drawKeypoints(gray,kp,img)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(3.4.3) C:\\projects\\opencv-python\\opencv_contrib\\modules\\xfeatures2d\\src\\sift.cpp:1207: error: (-213:The function/feature is not implemented) This algorithm is patented and is excluded in this configuration; Set OPENCV_ENABLE_NONFREE CMake option and rebuild the library in function 'cv::xfeatures2d::SIFT::create'\n"
     ]
    }
   ],
   "source": [
    "X=[]\n",
    "y=[]\n",
    "for i in range(len(training_data)):\n",
    "    img=sift_fun(training_data[i][0])\n",
    "    X.append(img)\n",
    "    y.append(trainin_data[i][1])\n",
    "labels_test=knn(X,y,X)\n",
    "count=0\n",
    "for i in range(len(y)):\n",
    "    if y[i]==labels_test[i]:\n",
    "        count+=1\n",
    "print(\"acc=\",count/(len(y)))\n",
    "print(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
